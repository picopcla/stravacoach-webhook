import os
import json
import io
from datetime import datetime, timedelta
from dateutil import parser
from flask import Flask, render_template, request, redirect
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload, MediaIoBaseUpload
from openai import OpenAI
from dotenv import load_dotenv
import numpy as np
import requests
import time
from xgboost import XGBClassifier
import pickle
import pandas as pd
import subprocess


# -------------------
# Fonction pour loguer les √©tapes avec dur√©e
# -------------------
def log_step(message, start_time):
    elapsed = time.time() - start_time
    print(f"‚è±Ô∏è {message} ‚Äî {elapsed:.2f} sec depuis d√©but")

app = Flask(__name__)
FOLDER_ID = '1OvCqOHHiOZoCOQtPaSwGoioR92S8-U7t'

# --- Chargement .env local si pas sur Render ---
if not os.getenv("RENDER"):
    load_dotenv(r"C:\StravaSecurity\main.env")
    print("‚úÖ Variables d'environnement charg√©es depuis main.env")

# --- Init OpenAI ---
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("OPENAI_API_KEY non d√©fini")
client = OpenAI(api_key=openai_api_key)
print("‚úÖ OpenAI client initialis√©")

# --- Init Google Drive ---
try:
    # Si Render, la variable est un JSON complet en string
    service_account_info = json.loads(os.environ['GOOGLE_APPLICATION_CREDENTIALS_JSON'])
except KeyError:
    # Sinon, on lit le fichier local
    with open(r'C:\StravaSecurity\services.json', 'r', encoding='utf-8') as f:
        service_account_info = json.load(f)

credentials = service_account.Credentials.from_service_account_info(
    service_account_info,
    scopes=['https://www.googleapis.com/auth/drive']
)
drive_service = build('drive', 'v3', credentials=credentials)
print("‚úÖ Google Drive service initialis√©")

# -------------------
# Helpers Google Drive
# ------------------
def load_file_from_drive(filename):
    try:
        results = drive_service.files().list(
            q=f"'{FOLDER_ID}' in parents and name='{filename}' and trashed=false",
            spaces='drive', fields='files(id, name)').execute()
        files = results.get('files', [])
        if not files:
            return {}
        file_id = files[0]['id']
        request_file = drive_service.files().get_media(fileId=file_id)
        fh = io.BytesIO()
        downloader = MediaIoBaseDownload(fh, request_file)
        done = False
        while not done:
            status, done = downloader.next_chunk()
        fh.seek(0)
        if filename.endswith('.json'):
            return json.loads(fh.read().decode("utf-8", errors="replace"))
        else:
            return fh.read().decode("utf-8", errors="replace")
    except Exception as e:
        print(f"Erreur chargement {filename}:", e)
        return {} if filename.endswith('.json') else ""

def upload_json_content_to_drive(json_data, drive_file_name):
    fh = io.BytesIO()
    fh.write(json.dumps(json_data, indent=2).encode("utf-8"))
    fh.seek(0)
    results = drive_service.files().list(
        q=f"'{FOLDER_ID}' in parents and name='{drive_file_name}' and trashed=false",
        spaces='drive', fields='files(id, name)', supportsAllDrives=True
    ).execute()
    files = results.get('files', [])
    media = MediaIoBaseUpload(fh, mimetype='application/json')
    if files:
        file_id = files[0]['id']
        drive_service.files().update(
            fileId=file_id, media_body=media, supportsAllDrives=True
        ).execute()
    else:
        metadata = {
            'name': drive_file_name,
            'parents': [FOLDER_ID],
            'mimeType': 'application/json'
        }
        drive_service.files().create(
            body=metadata, media_body=media, fields='id', supportsAllDrives=True
        ).execute()
        
def save_short_term_objectives(data):
    upload_json_content_to_drive(data, 'short_term_objectives.json')
    
# -------------------
# D√©tection du type de s√©ance (r√®gles simples par distance)
# -------------------
def detect_session_type(activity):
    """
    R√®gles par distance (aucune substitution par XGBoost ici) :
    - > 11 km  -> long_run
    - < 8 km   -> normal_5k
    - sinon    -> normal_10k
    """
    pts = activity.get("points", [])
    if not pts:
        return activity.get("type_sortie", "inconnue") or "inconnue"

    dist_km = pts[-1]["distance"] / 1000.0

    if dist_km > 11:
        return "long_run"
    if dist_km < 8:
        return "normal_5k"
    return "normal_10k"

    
def _features_fractionne(activity):
    """Vecteur de 4 features simples pour XGBoost: [cv_allure, cv_fc, blocs_rapides, pct_temps_90].
    - blocs_rapides d√©tect√©s avec une fen√™tre glissante en distance (offset libre)
    """
    pts = activity.get("points", [])
    if len(pts) < 10:
        return [0.0, 0.0, 0.0, 0.0]

    fcs = np.array([p.get("hr") for p in pts], dtype=float)
    vels = np.array([p.get("vel") for p in pts], dtype=float)
    dists = np.array([p.get("distance") for p in pts], dtype=float)

    # Allures en min/km (nan si vel <= 0)
    allures = np.where(vels > 0, (1.0 / vels) * 16.6667, np.nan)

    # 1) CV allure & 2) CV FC
    def _cv(x):
        m = np.nanmean(x)
        if not np.isfinite(m) or m == 0:
            return 0.0
        return float(np.nanstd(x) / m)

    cv_allure = _cv(allures)
    cv_fc = _cv(fcs)

    # 3) Nombre de blocs rapides d√©tect√©s via fen√™tre glissante
    WINDOW_M   = 500     # longueur de la fen√™tre en m√®tres
    FAST_DELTA = 0.40    # seuil de rapidit√© (min/km plus rapide que la moyenne)
    COOLDOWN_M = 200     # distance minimale entre deux d√©tections pour √©viter les doublons

    mean_all = np.nanmean(allures)
    thr_fast = mean_all - FAST_DELTA if np.isfinite(mean_all) else np.nan

    blocs_rapides = 0
    i = 0
    N = len(dists)
    last_hit_end_d = -1e9  # distance fin du dernier bloc valid√©

    while i < N - 1:
        # trouve j tel que distance(i ‚Üí j) >= WINDOW_M
        j = i
        while j < N and (dists[j] - dists[i]) < WINDOW_M:
            j += 1
        if j >= N:
            break

        bloc_all = np.nanmean(allures[i:j+1])
        dist_i, dist_j = dists[i], dists[j]

        # Crit√®re rapide + cooldown respect√©
        is_fast = (np.isfinite(bloc_all) and np.isfinite(thr_fast) and bloc_all < thr_fast)
        far_enough = (dist_i - last_hit_end_d) >= COOLDOWN_M

        if is_fast and far_enough:
            blocs_rapides += 1
            last_hit_end_d = dist_j
            # saute √† la fin du bloc + cooldown
            i = j
            while i < N and (dists[i] - last_hit_end_d) < COOLDOWN_M:
                i += 1
            continue

        # sinon avance juste d'un point
        i += 1

    # 4) % temps au-dessus de 90% FCmax de la s√©ance
    if np.all(np.isnan(fcs)) or len(fcs) == 0:
        pct_90 = 0.0
    else:
        fcmax = np.nanmax(fcs)
        thr = 0.9 * fcmax if np.isfinite(fcmax) and fcmax > 0 else np.nan
        if np.isfinite(thr):
            pct_90 = float(np.nansum(fcs > thr) / np.count_nonzero(~np.isnan(fcs)))
        else:
            pct_90 = 0.0

    feats = [cv_allure, cv_fc, float(blocs_rapides), pct_90]
    # Remplace NaN/¬±inf par 0 pour le mod√®le
    feats = [0.0 if (not np.isfinite(v)) else float(v) for v in feats]
    return feats

    
    
def tag_session_types(activities):
    changed = False
    for act in activities:
        cur = act.get("type_sortie")
        if cur in (None, "-", "inconnue") or act.get("force_recompute", False):
            new_type = detect_session_type(act)
            if new_type != cur:
                act["type_sortie"] = new_type
                changed = True
        act.pop("force_recompute", None)
    return activities, changed
def apply_fractionne_flags(activities):
    """
    Ajoute/actualise :
      - is_fractionne: bool
      - fractionne_prob: float [0..1]
    Sans jamais modifier type_sortie.
    R√®gle: on √©value XGB seulement si distance <= 11 km ; sinon is_fractionne=False.
    PRIORIT√â aux labels manuels (is_fractionne_label) s'ils sont pr√©sents.
    """
    changed = False
    for act in activities:
        # 1) Priorit√© au label manuel
        if "is_fractionne_label" in act:
            lbl = bool(act["is_fractionne_label"])
            new_flag = lbl
            new_prob = 1.0 if lbl else 0.0

        else:
            # 2) Sinon, on calcule avec le mod√®le si possible
            pts = act.get("points", [])
            if not pts:
                new_flag, new_prob = False, 0.0
            else:
                dist_km = pts[-1].get("distance", 0) / 1000.0
                if dist_km > 11 or fractionne_model is None:
                    new_flag, new_prob = False, 0.0
                else:
                    feats = _features_fractionne(act)
                    try:
                        proba = float(fractionne_model.predict_proba(np.array([feats]))[0][1])
                        new_prob = round(proba, 3)
                        new_flag = (proba >= 0.5)  # seuil ajustable
                    except Exception as e:
                        print("ü§ñ XGBoost predict error:", e)
                        new_flag, new_prob = False, 0.0

        # 3) Appliquer si changement
        if act.get("is_fractionne") != new_flag or act.get("fractionne_prob") != new_prob:
            act["is_fractionne"] = new_flag
            act["fractionne_prob"] = new_prob
            changed = True

    return activities, changed




print("‚úÖ Helpers OK")


# -------- XGBoost fractionn√© (chargement mod√®le) --------
MODEL_PATH = "fractionne_xgb.pkl"
fractionne_model = None  # global lecture seule

# ==== Auto-r√©entrainement XGBoost ====
AUTO_RETRAIN_XGB = True                 # d√©sactive en mettant False si besoin
LAST_TRAIN_META = "ml/.last_train_meta.json"  # fichier local pour m√©moriser le dernier √©tat (compte d‚Äôactivit√©s)

def _load_last_train_meta():
    try:
        with open(LAST_TRAIN_META, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {"last_trained_count": 0}

def _save_last_train_meta(count):
    os.makedirs(os.path.dirname(LAST_TRAIN_META), exist_ok=True)
    with open(LAST_TRAIN_META, "w", encoding="utf-8") as f:
        json.dump({"last_trained_count": int(count)}, f)

def _count_manual_labels(activities):
    pos = neg = 0
    for a in activities:
        if "is_fractionne_label" in a:
            if bool(a["is_fractionne_label"]): pos += 1
            else: neg += 1
    return pos, neg

def _should_retrain_xgb(activities):
    """
    Vrai si:
      - on a au moins 8 fractionn√©s (pos) ET 8 non-fractionn√©s (neg), ET
      - au moins 1 nouvelle activit√© depuis le dernier entra√Ænement.
    """
    meta = _load_last_train_meta()
    last_cnt = meta.get("last_trained_count", 0)
    cur_cnt = len(activities)
    if cur_cnt <= last_cnt:
        return False

    pos, neg = _count_manual_labels(activities)
    if pos < 8 or neg < 8:
        print(f"‚ÑπÔ∏è Pas assez de labels pour auto-train (pos={pos}, neg={neg}, min=8 chacun)")
        return False

    print(f"üîÅ Auto-train √©ligible: new_activities={cur_cnt - last_cnt}, labels(pos={pos}, neg={neg})")
    return True

def _retrain_fractionne_model_and_reload():
    """
    Lance ml/train_fractionne_xgb.py, recharge le mod√®le, m√©morise le nb d'activit√©s.
    """
    try:
        print("ü§ñ Auto-train: lancement ml/train_fractionne_xgb.py ...")
        subprocess.run(["python", "ml/train_fractionne_xgb.py"], check=True, timeout=300)
        # Recharge le mod√®le
        global fractionne_model
        fractionne_model = load_fractionne_model()
        # M√©morise le nouveau compteur
        activities = load_activities()
        _save_last_train_meta(len(activities))
        print("‚úÖ Auto-train OK et mod√®le recharg√©.")
        return True
    except Exception as e:
        print("‚ùå Auto-train √©chou√©:", e)
        return False


def load_fractionne_model(path=MODEL_PATH):
    try:
        with open(path, "rb") as f:
            model = pickle.load(f)
        print(f"ü§ñ XGBoost fractionn√© charg√©: {path}")
        return model
    except FileNotFoundError:
        print("ü§ñ XGBoost fractionn√© introuvable ‚Üí d√©sactiv√© (pas de fichier .pkl).")
    except Exception as e:
        print("ü§ñ Erreur chargement XGBoost:", e)
    return None

# Charge √† l'init
fractionne_model = load_fractionne_model()

# -------------------
# Fonction m√©t√©o (Open-Meteo)
# -------------------
from datetime import datetime, timedelta, date
import requests
from collections import Counter

from dateutil import parser  # d√©j√† import√©

def get_temperature_for_run(lat, lon, start_datetime_str, duration_minutes):
    try:
        # ‚úÖ Parse ISO 8601 (Z ou +02:00)
        start_dt = parser.isoparse(start_datetime_str)
        end_dt = start_dt + timedelta(minutes=duration_minutes)

        # ‚úÖ Supprime le fuseau pour comparer avec les donn√©es na√Øves de l'API
        start_dt = start_dt.replace(tzinfo=None)
        end_dt = end_dt.replace(tzinfo=None)

        print(f"üïí Heure d√©but (start_dt): {start_dt}, fin (end_dt): {end_dt}")
    except Exception as e:
        print("‚ùå Erreur parsing datetime pour m√©t√©o:", e, start_datetime_str)
        return None, None, None, None

    today = date.today()
    yesterday = today - timedelta(days=1)
    is_today = start_dt.date() == today
    is_yesterday = start_dt.date() == yesterday

   # ‚úÖ Utilise forecast pour aujourd'hui et hier
    if is_today or is_yesterday:
        query_type = "forecast"
        url = (
            f"https://api.open-meteo.com/v1/forecast?"
            f"latitude={lat}&longitude={lon}"
            f"&hourly=temperature_2m,weathercode"
            f"&timezone=auto"
        )
    else:
        query_type = "archive"
        # Archive pour avant-hier et plus
        date_str = start_dt.strftime("%Y-%m-%d")
        url = (
            f"https://archive-api.open-meteo.com/v1/archive?"
            f"latitude={lat}&longitude={lon}"
            f"&start_date={date_str}&end_date={date_str}"
            f"&hourly=temperature_2m,weathercode"
            f"&timezone=auto"
        )

    try:
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()

        hours = data.get("hourly", {}).get("time", [])
        temps = data.get("hourly", {}).get("temperature_2m", [])
        weathercodes = data.get("hourly", {}).get("weathercode", [])

        if not hours or not temps:
            print("‚ö†Ô∏è Aucune donn√©e horaire trouv√©e.")
            return None, None, None, None

        # Convertit toutes les heures en datetime (na√Øves) pour comparaison
        hours_dt = [datetime.fromisoformat(h) for h in hours]

        # Trouver la temp√©rature la plus proche pour d√©but et fin
        def closest_temp(target_dt):
            diffs = [abs((dt - target_dt).total_seconds()) for dt in hours_dt]
            idx = diffs.index(min(diffs))
            return temps[idx] if temps[idx] is not None else None

        temp_debut = closest_temp(start_dt)
        temp_fin = closest_temp(end_dt)

        # Moyenne sur la fen√™tre de course
        temp_values = [
            temp for dt, temp in zip(hours_dt, temps)
            if start_dt <= dt <= end_dt and temp is not None
        ]

        # ‚úÖ Si pas de moyenne, utiliser au moins temp_debut ou temp_fin
        avg_temp = (
            round(sum(temp_values) / len(temp_values), 1)
            if temp_values else temp_debut or temp_fin
        )

        # Code m√©t√©o le plus fr√©quent pendant la course
# ‚úÖ Trouver le code m√©t√©o dominant avec une marge de 30 min

        margin = timedelta(minutes=30)
        weather_in_window = [
            wc for dt, wc in zip(hours_dt, weathercodes)
            if (start_dt - margin) <= dt <= (end_dt + margin) and wc is not None
        ]

        if weather_in_window:
            # Si on a trouv√© des codes m√©t√©o dans la fen√™tre √©largie, on prend le plus fr√©quent
            most_common_code = Counter(weather_in_window).most_common(1)[0][0]
        else:
            # Sinon, on prend le code m√©t√©o le plus proche du d√©but de la course
            diffs = [abs((dt - start_dt).total_seconds()) for dt in hours_dt]
            most_common_code = weathercodes[diffs.index(min(diffs))] if diffs else None

        return avg_temp, temp_debut, temp_fin, most_common_code

    except Exception as e:
        print("‚ùå Erreur lors de la requ√™te ou du traitement m√©t√©o:", e)
        return None, None, None, None



def get_weather_emoji_for_activity(activity):
    weather_code_map = {
        0: "‚òÄÔ∏è", 1: "üå§Ô∏è", 2: "‚õÖ", 3: "‚òÅÔ∏è",
        45: "üå´Ô∏è", 48: "üå´Ô∏è", 51: "üå¶Ô∏è", 53: "üåßÔ∏è",
        55: "üåßÔ∏è", 61: "üåßÔ∏è", 63: "üåßÔ∏è", 65: "üåßÔ∏è",
        71: "‚ùÑÔ∏è", 73: "‚ùÑÔ∏è", 75: "‚ùÑÔ∏è", 80: "üåßÔ∏è",
        81: "üåßÔ∏è", 82: "üåßÔ∏è", 95: "‚õàÔ∏è", 96: "‚õàÔ∏è",
        99: "‚õàÔ∏è"
    }
    points = activity.get("points", [])
    if not points:
        return "‚ùì"
    lat, lon = None, None
    if "lat" in points[0] and "lng" in points[0]:
        lat, lon = points[0]["lat"], points[0]["lng"]
    elif "start_latlng" in activity and activity["start_latlng"]:
        lat, lon = activity["start_latlng"][0], activity["start_latlng"][1]
    date_str = activity.get("date", None)
    if not lat or not lon or not date_str:
        return "‚ùì"
    duration_minutes = (points[-1]["time"] - points[0]["time"]) / 60
    _, _, _, weather_code = get_temperature_for_run(lat, lon, date_str, duration_minutes)
    return weather_code_map.get(weather_code, "‚ùì")
    
def ensure_weather_data(activities):
    """V√©rifie que chaque activit√© a les donn√©es m√©t√©o et les calcule si elles sont absentes."""
    updated = False

    for act in activities:
        if act.get("avg_temperature") is None or act.get("weather_code") is None:
            points = act.get("points", [])
            if not points:
                continue

            lat, lon = points[0].get("lat"), points[0].get("lng")
            duration = (points[-1]["time"] - points[0]["time"]) / 60

            avg_temp, _, _, weather_code = get_temperature_for_run(
                lat, lon, act.get("date"), duration
            )

            act["avg_temperature"] = avg_temp
            act["weather_code"] = weather_code
            updated = True
            print(f"üå§Ô∏è M√©t√©o ajout√©e pour {act.get('date')} ‚ûú {avg_temp}¬∞C / code {weather_code}")

    if updated:
        upload_json_content_to_drive(activities, 'activities.json')
        print("üíæ activities.json mis √† jour avec la m√©t√©o")

    return activities


# -------------------
# Loaders
# -------------------
def load_activities(): return load_file_from_drive('activities.json') or []
def load_profile(): return load_file_from_drive('profile.json') or {"birth_date": "", "weight": 0, "events": []}
def load_objectives(): return load_file_from_drive('objectives.json') or {}
def load_short_term_prompt_from_drive():
    return load_file_from_drive('prompt_short_term.txt') or "Donne directement le JSON des objectifs √† court terme."
def load_short_term_objectives(): return load_file_from_drive('short_term_objectives.json') or {}

# -------------------
# Fonctions sp√©cifiques (inchang√©es sauf enrich_activities etc)
# -------------------
def get_fcmax_from_fractionnes(activities):
    fcmax = 0
    for act in activities:
        if act.get("type_sortie") == "fractionn√©" or act.get("is_fractionne") is True:
            for point in act.get("points", []):
                hr = point.get("hr")
                if hr is not None and hr > fcmax:
                    fcmax = hr
    return fcmax

def enrich_single_activity(activity, fc_max_fractionnes):
    points = activity.get("points", [])
    if not points or len(points) < 5:
        return activity

    distances = np.array([p["distance"]/1000 for p in points])
    fcs = np.array([p.get("hr") if p.get("hr") is not None else np.nan for p in points])
    vels = np.array([p.get("vel", 0) for p in points])
    alts = np.array([p.get("alt", 0) for p in points])

    delta_dist = np.diff(distances, prepend=distances[0]) * 1000
    delta_alt = np.diff(alts, prepend=alts[0])
    delta_dist[delta_dist == 0] = 0.001

    pentes = (delta_alt / delta_dist) * 100
    allures_brutes = np.where(vels > 0, (1 / vels) * 16.6667, np.nan)
    allures_corrigees = np.where((allures_brutes - 0.2 * pentes) < 0, np.nan, allures_brutes - 0.2 * pentes)

    ratios = np.where(allures_corrigees > 0, fcs / allures_corrigees, np.nan)
    valid = (~np.isnan(allures_corrigees)) & (~np.isnan(fcs))
    distances, fcs, allures_corrigees, ratios = distances[valid], fcs[valid], allures_corrigees[valid], ratios[valid]

    if len(distances) < 5:
        return activity

    total_duration = points[-1]["time"] - points[0]["time"]
    slope, intercept = np.polyfit(distances, ratios, 1)
    r_squared = np.corrcoef(distances, ratios)[0,1]**2
    collapse_threshold = np.mean(allures_corrigees[:max(1,len(allures_corrigees)//3)]) * 1.10
    collapse_distance = next((d for a, d in zip(allures_corrigees, distances) if a > collapse_threshold), distances[-1])
    cv_allure = np.std(allures_corrigees) / np.mean(allures_corrigees)
    cv_cardio = np.std(ratios) / np.mean(ratios)
    seuil_90 = 0.9 * fc_max_fractionnes
    above_90_count = sum(1 for hr in fcs if hr > seuil_90)
    time_above_90 = (above_90_count / len(fcs)) * total_duration if len(fcs) else 0
    split = max(1, len(allures_corrigees)//3)
    endurance_index = np.mean(allures_corrigees[-split:]) / np.mean(allures_corrigees[:split])
    fc_moy, allure_moy = np.mean(fcs), np.mean(allures_corrigees)
    k_moy = 0.43 * (fc_moy / allure_moy) - 5.19 if allure_moy > 0 else "-"
    ratio_first, ratio_last = np.mean(ratios[:split]), np.mean(ratios[-split:])
    deriv_cardio = ratio_last / ratio_first if ratio_first > 0 else "-"
    seuil_bas, seuil_haut = 0.6 * fc_max_fractionnes, 0.7 * fc_max_fractionnes
    zone2_count = sum(1 for hr in fcs if seuil_bas < hr < seuil_haut)
    pourcentage_zone2 = (zone2_count / len(fcs)) * 100 if len(fcs) else 0
    ratio_fc_allure_global = np.mean(ratios)

    activity.update({
        "drift_slope": round(slope, 4),
        "drift_r2": round(r_squared, 4),
        "collapse_distance_km": round(collapse_distance, 2),
        "cv_allure": round(cv_allure, 4),
        "cv_cardio": round(cv_cardio, 4),
        "time_above_90_pct_fcmax": round(time_above_90, 1),
        "endurance_index": round(endurance_index, 4),
        "k_moy": round(k_moy, 3) if isinstance(k_moy, float) else "-",
        "deriv_cardio": round(deriv_cardio, 3) if isinstance(deriv_cardio, float) else "-",
        "pourcentage_zone2": round(pourcentage_zone2, 1),
        "ratio_fc_allure_global": round(ratio_fc_allure_global, 3)
    })

    return activity

def enrich_activities(activities):
    fc_max_fractionnes = get_fcmax_from_fractionnes(activities)
    print(f"üìà FC max fractionn√©s: {fc_max_fractionnes}")

    for idx, activity in enumerate(activities):
        # 1) Assigner le type de s√©ance si manquant/forc√© (r√®gles simples par distance)
        if activity.get("type_sortie") in (None, "-", "inconnue") or activity.get("force_recompute", False):
            activity["type_sortie"] = detect_session_type(activity)

        # 2) Enrichissements num√©riques (k, d√©rive cardio, etc.)
        activity = enrich_single_activity(activity, fc_max_fractionnes)

        print(f"üèÉ Act#{idx+1} ‚ûî type: {activity.get('type_sortie')}, k_moy: {activity.get('k_moy')}")
        activity.pop("force_recompute", None)

    return activities


def allure_mmss_to_decimal(mmss):
    try:
        minutes, seconds = mmss.split(":")
        return int(minutes) + int(seconds) / 60
    except Exception:
        return 0.0
        
def convert_short_term_allures(short_term):
    if not short_term or "prochains_runs" not in short_term:
        return short_term
    for run in short_term["prochains_runs"]:
        if isinstance(run.get("allure"), str):
            run["allure_decimal"] = allure_mmss_to_decimal(run["allure"])
        else:
            run["allure_decimal"] = 0.0
    return short_term

print("‚úÖ Activities OK")

# -------------------
# Dashboard principal
# -------------------
def compute_dashboard_data(activities):
    weather_code_map = {
       0: "‚òÄÔ∏è",  # Clear sky
       1: "üå§Ô∏è",  # Mainly clear
       2: "‚õÖ",   # Partly cloudy
       3: "‚òÅÔ∏è",  # Overcast
       45: "üå´Ô∏è", # Fog
       48: "üå´Ô∏è", # Depositing rime fog
       51: "üå¶Ô∏è", # Drizzle light
       53: "üåßÔ∏è", # Drizzle moderate
       55: "üåßÔ∏è", # Drizzle dense
       61: "üåßÔ∏è", # Rain slight
       63: "üåßÔ∏è", # Rain moderate
       65: "üåßÔ∏è", # Rain heavy
       71: "‚ùÑÔ∏è",  # Snow fall slight
       73: "‚ùÑÔ∏è",  # Snow fall moderate
       75: "‚ùÑÔ∏è",  # Snow fall heavy
       80: "üåßÔ∏è", # Rain showers slight
       81: "üåßÔ∏è", # Rain showers moderate
       82: "üåßÔ∏è", # Rain showers violent
       95: "‚õàÔ∏è",  # Thunderstorm slight
       96: "‚õàÔ∏è",  # Thunderstorm with slight hail
       99: "‚õàÔ∏è",  # Thunderstorm with heavy hail
    }

    print("\nüîç DEBUG --- V√©rification temp√©rature")

    activities.sort(key=lambda x: x.get("date"))
    last = activities[-1] if activities else {}

    points = last.get("points", [])
    if not points:
        print("‚ö†Ô∏è Pas de points dans la derni√®re activit√©")
        return {}

    # Date
    date_str = "-"
    try:
        date_str = datetime.strptime(last.get("date"), "%Y-%m-%dT%H:%M:%S%z").strftime("%Y-%m-%d")
    except Exception as e:
        print("‚ùå Erreur parsing date:", e)
        date_str = None
    print("üìÖ Date activit√©:", date_str)

    # GPS
    lat, lon = None, None
    if points and "lat" in points[0] and "lng" in points[0]:
        lat, lon = points[0]["lat"], points[0]["lng"]
    elif "start_latlng" in last and last["start_latlng"]:
        lat, lon = last["start_latlng"][0], last["start_latlng"][1]

   # Temp√©rature : utiliser la m√©t√©o d√©j√† stock√©e si disponible
    avg_temperature = last.get("avg_temperature")
    weather_code = last.get("weather_code")
    temp_debut = avg_temperature
    temp_fin = avg_temperature

    if lat is not None and lon is not None and date_str:
        # Si m√©t√©o absente, on la calcule une seule fois et on la sauvegarde
        if avg_temperature is None or weather_code is None:
            start_datetime_str = last.get("date")  # Ex: "2025-07-18T19:45:57Z"
            duration_minutes = (points[-1]["time"] - points[0]["time"]) / 60 if points else 0

            avg_temperature, temp_debut, temp_fin, weather_code = get_temperature_for_run(
                lat, lon, start_datetime_str, duration_minutes
            )

            # Sauvegarde dans l'activit√©
            last["avg_temperature"] = avg_temperature
            last["weather_code"] = weather_code

            # Met √† jour activities.json pour √©viter un recalcul futur
            upload_json_content_to_drive(activities, 'activities.json')

            print(f"üå°Ô∏è Temp√©rature calcul√©e et sauvegard√©e : {avg_temperature}¬∞C")
        else:
            print(f"üå°Ô∏è Temp√©rature lue depuis activities.json : {avg_temperature}¬∞C")
    else:
        print("‚ö†Ô∏è Impossible d‚Äôappeler m√©t√©o: coordonn√©es ou date manquantes.")

    # Si aucun code m√©t√©o n‚Äôest disponible, on force un fallback
    if weather_code is None:
        weather_code = -1


    weather_emoji = weather_code_map.get(weather_code, "‚ùì")

    # Metrics
    total_dist = points[-1]["distance"] / 1000
    total_time = (points[-1]["time"] - points[0]["time"]) / 60
    allure_moy = total_time / total_dist if total_dist > 0 else None
    hr_vals = [p["hr"] for p in points if p.get("hr")]
    labels = [round(p["distance"] / 1000, 3) for p in points]
    if labels and labels[0] != 0:
        labels[0] = 0.0
    points_fc = [p["hr"] for p in points if p.get("hr") is not None]
    points_alt = [p["alt"] - points[0]["alt"] for p in points if p.get("alt") is not None]

    # Allure curve
    allure_curve = []
    bloc_start_idx, next_bloc_dist, last_allure = 0, 500, None
    for i, p in enumerate(points):
        if p["distance"] >= next_bloc_dist or i == len(points) - 1:
            bloc_points = points[bloc_start_idx:i + 1]
            bloc_dist = bloc_points[-1]["distance"] - bloc_points[0]["distance"]
            bloc_time = bloc_points[-1]["time"] - bloc_points[0]["time"]
            if bloc_dist > 0:
                last_allure = (bloc_time / 60) / (bloc_dist / 1000)
            allure_curve.extend([last_allure] * len(bloc_points))
            bloc_start_idx = i + 1
            next_bloc_dist += 500
    while len(allure_curve) < len(points):
        allure_curve.append(last_allure)

    print("üìä Dashboard calcul√©")

    return {
        "type_sortie": last.get("type_sortie", "-"),
        "date": date_str,
        "distance_km": round(total_dist, 2),
        "duration_min": round(total_time, 1),
        "allure": f"{int(allure_moy)}:{int((allure_moy - int(allure_moy)) * 60):02d}" if allure_moy else "-",
        "fc_moy": round(sum(hr_vals) / len(hr_vals), 1) if hr_vals else "-",
        "fc_max": max(hr_vals) if hr_vals else "-",
        "k_moy": last.get("k_moy", "-"),
        "deriv_cardio": last.get("deriv_cardio", "-"),
        "gain_alt": round(points[-1]["alt"] - points[0]["alt"], 1),
        "drift_slope": last.get("drift_slope", "-"),
        "cv_allure": last.get("cv_allure", "-"),
        "cv_cardio": last.get("cv_cardio", "-"),
        "collapse_distance_km": last.get("collapse_distance_km", "-"),
        "pourcentage_zone2": last.get("pourcentage_zone2", "-"),
        "time_above_90_pct_fcmax": last.get("time_above_90_pct_fcmax", "-"),
        "ratio_fc_allure_global": last.get("ratio_fc_allure_global", "-"),
        "avg_temperature": avg_temperature,
        "temp_debut": temp_debut,
        "temp_fin": temp_fin,
        "labels": json.dumps(labels),
        "allure_curve": json.dumps(allure_curve),
        "points_fc": json.dumps(points_fc),
        "points_alt": json.dumps(points_alt),
        "history_dates": json.dumps([a["date"][:10] for a in activities if a.get("k_moy") != "-"]),
        "history_k": json.dumps([a["k_moy"] for a in activities if a.get("k_moy") != "-"]),
        "temperature": avg_temperature,
        "weather_code": weather_code,
        "weather_emoji": weather_emoji,
        "history_drift": json.dumps([a["deriv_cardio"] for a in activities if a.get("deriv_cardio") != "-"]),
    }

@app.route("/")
def index():
    start_time = time.time()
    log_step("D√©but index()", start_time)

    # Charger les activit√©s
    activities = load_activities()

    # V√©rifier si certaines activit√©s sont incompl√®tes
    needs_weather = any(
        act.get("avg_temperature") is None or act.get("weather_code") is None
        for act in activities
    )
    needs_enrich = any(
        act.get("k_moy") in (None, "-") or act.get("deriv_cardio") in (None, "-")
        for act in activities
    )
    needs_session = any(
    act.get("type_sortie") in (None, "-", "inconnue") for act in activities
    )


    modified = False
    if needs_weather:
        print("üå§Ô∏è M√©t√©o manquante ‚Üí calcul m√©t√©o")
        activities = ensure_weather_data(activities)
        modified = True
    
    if needs_session:
        print("üè∑Ô∏è Session type manquant ‚Üí tagging par r√®gles")
        activities, changed = tag_session_types(activities)
        modified = modified or changed
        
        # Auto-r√©entrainement XGB si nouvelle activit√© + labels suffisants
    if AUTO_RETRAIN_XGB and _should_retrain_xgb(activities):
        _retrain_fractionne_model_and_reload()

        
    print("ü§ñ Marquage fractionn√© (is_fractionne / fractionne_prob)")
    activities, changed = apply_fractionne_flags(activities)
    modified = modified or changed

    if needs_enrich:
        print("üìà Enrichissement manquant ‚Üí enrichissement")
        activities = enrich_activities(activities)
        modified = True

    if modified:
        upload_json_content_to_drive(activities, 'activities.json')
        print("üíæ activities.json mis √† jour apr√®s compl√©tion")

    log_step("Activities charg√©es et compl√©t√©es", start_time)
    print(f"üìÇ {len(activities)} activit√©s pr√™tes")

    # Calcul du dashboard
    dashboard = compute_dashboard_data(activities)
    log_step("Dashboard calcul√©", start_time)

    activities_for_carousel = []

    # Construction du carrousel (inchang√©)
    for act in reversed(activities[-10:]):  # 10 derni√®res activit√©s
        log_step(f"D√©but carrousel activit√© {act.get('date')}", start_time)
        points = act.get("points", [])
        if not points:
            continue

        labels = [round(p["distance"] / 1000, 3) for p in points]
        points_fc = [p.get("hr", 0) for p in points]
        points_alt = [p.get("alt", 0) - points[0].get("alt", 0) for p in points]

        # Calcul allure_curve tous les 500m
        allure_curve = []
        bloc_start_idx, next_bloc_dist, last_allure = 0, 500, None
        for i, p in enumerate(points):
            if p["distance"] >= next_bloc_dist or i == len(points) - 1:
                bloc_points = points[bloc_start_idx:i + 1]
                bloc_dist = bloc_points[-1]["distance"] - bloc_points[0]["distance"]
                bloc_time = bloc_points[-1]["time"] - bloc_points[0]["time"]
                if bloc_dist > 0:
                    last_allure = (bloc_time / 60) / (bloc_dist / 1000)
                allure_curve.extend([last_allure] * len(bloc_points))
                bloc_start_idx = i + 1
                next_bloc_dist += 500
        while len(allure_curve) < len(points):
            allure_curve.append(last_allure)

        # Statistiques globales
        total_dist_km = points[-1]["distance"] / 1000
        total_time_min = (points[-1]["time"] - points[0]["time"]) / 60
        allure_moy = total_time_min / total_dist_km if total_dist_km > 0 else None
        fc_max = max(points_fc) if points_fc else None
        gain_alt = round(points[-1]["alt"] - points[0]["alt"], 1)

        # üå°Ô∏è M√©t√©o
        avg_temperature = act.get("avg_temperature")
        weather_code = act.get("weather_code")
        weather_code_map = {
            0: "‚òÄÔ∏è", 1: "üå§Ô∏è", 2: "‚õÖ", 3: "‚òÅÔ∏è",
            45: "üå´Ô∏è", 48: "üå´Ô∏è", 51: "üå¶Ô∏è", 53: "üåßÔ∏è",
            55: "üåßÔ∏è", 61: "üåßÔ∏è", 63: "üåßÔ∏è", 65: "üåßÔ∏è",
            71: "‚ùÑÔ∏è", 73: "‚ùÑÔ∏è", 75: "‚ùÑÔ∏è", 80: "üåßÔ∏è",
            81: "üåßÔ∏è", 82: "üåßÔ∏è", 95: "‚õàÔ∏è", 96: "‚õàÔ∏è", 99: "‚õàÔ∏è"
        }
        weather_emoji = weather_code_map.get(weather_code, "‚ùì")

        # Date format√©e
        try:
            date_formatted = datetime.strptime(
                act.get("date"), "%Y-%m-%dT%H:%M:%S%z"
            ).strftime("%Y-%m-%d")
        except:
            date_formatted = "-"

        activities_for_carousel.append({
            "date": date_formatted,
            "type_sortie": act.get("type_sortie", "-"),
            "is_fractionne": act.get("is_fractionne", False),
            "fractionne_prob": act.get("fractionne_prob", 0.0), 
            "distance_km": round(total_dist_km, 2),
            "duration_min": round(total_time_min, 1),
            "fc_moy": round(np.mean(points_fc), 1) if points_fc else "-",
            "fc_max": fc_max,
            "allure": f"{int(allure_moy)}:{int((allure_moy - int(allure_moy)) * 60):02d}" if allure_moy else "-",
            "gain_alt": gain_alt,
            "k_moy": act.get("k_moy", "-"),
            "deriv_cardio": act.get("deriv_cardio", "-"),
            "temperature": avg_temperature,
            "weather_emoji": weather_emoji,
            "labels": json.dumps(labels),
            "points_fc": json.dumps(points_fc),
            "points_alt": json.dumps(points_alt),
            "allure_curve": json.dumps(allure_curve),
        })

    # Retourne la page
    return render_template(
        "index.html",
        dashboard=dashboard,
        objectives=load_objectives(),
        short_term=load_short_term_objectives(),
        activities_for_carousel=activities_for_carousel
    )

    
@app.route("/refresh")
def refresh():
    """Recalcule et met √† jour activities.json sur Drive"""
    print("‚ôªÔ∏è Recalcul des activit√©s...")
    activities = load_activities()
    activities = enrich_activities(activities)
    upload_json_content_to_drive(activities, 'activities.json')
    print("‚úÖ activities.json mis √† jour sur Drive")
    return "‚úÖ Donn√©es mises √† jour"
      
@app.route('/profile', methods=['GET','POST'])
def profile():
    profile = load_profile()
    if request.method == 'POST':
        profile['birth_date'] = request.form.get('birth_date', '')
        weight = request.form.get('weight', '')
        profile['weight'] = float(weight) if weight else 0.0
        profile['global_objective'] = request.form.get('global_objective', '')
        profile['particular_objective'] = request.form.get('particular_objective', '')

        # R√©cup√©rer listes des √©v√©nements : dates et noms
        event_dates = request.form.getlist('event_date')
        event_names = request.form.getlist('event_name')

        events = []
        for d, n in zip(event_dates, event_names):
            d = d.strip()
            n = n.strip()
            if d and n:
                events.append({'date': d, 'name': n})
        profile['events'] = events

        upload_json_content_to_drive(profile, 'profile.json')
        return redirect('/profile')

    return render_template('profile.html', profile=profile)
    
@app.route('/generate_short_term_plan')
def generate_short_term_plan():
    profile = load_profile()
    activities = load_activities()
    prompt_template = load_short_term_prompt_from_drive()

    # Exemple simple pour construire prompt
    prompt = prompt_template
    prompt += f"\nProfil: {profile}"
    prompt += f"\nActivit√©s r√©centes: {len(activities)}"

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",  
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=1000
        )
        result_str = response.choices[0].message.content.strip()

        # Essayer de parser la r√©ponse JSON (pr√©cise √† OpenAI de r√©pondre en JSON strict)
        short_term_objectives = json.loads(result_str)

        # Ajouter conversion allures au format d√©cimal si besoin
        short_term_objectives = convert_short_term_allures(short_term_objectives)

        save_short_term_objectives(short_term_objectives)

        print("‚úÖ Coaching court terme g√©n√©r√© et sauvegard√©.")

        return redirect('/')  # ou retourner un message / JSON si API

    except Exception as e:
        print("‚ùå Erreur g√©n√©ration coaching court terme:", e)
        return f"Erreur g√©n√©ration coaching: {e}", 500
        
@app.route("/recompute_session_types")
def recompute_session_types():
    """Recalcule le type_sortie de toutes les activit√©s avec la r√®gle par distance."""
    activities = load_activities()
    print(f"‚ôªÔ∏è Recalcul session_type pour {len(activities)} activit√©s")

    for act in activities:
        # Toujours recalculer, m√™me si d√©j√† d√©fini
        act["type_sortie"] = detect_session_type(act)

    upload_json_content_to_drive(activities, "activities.json")
    print("‚úÖ activities.json mis √† jour avec nouveaux session_type")
    return f"‚úÖ Recalcul√© pour {len(activities)} activit√©s"
    
@app.route("/recompute_fractionne_flags")
def recompute_fractionne_flags():
    activities = load_activities()
    activities, changed = apply_fractionne_flags(activities)
    if changed:
        upload_json_content_to_drive(activities, "activities.json")
        msg = f"‚úÖ Flags fractionn√© mis √† jour ({len(activities)} activit√©s)"
    else:
        msg = "‚ÑπÔ∏è Aucun changement sur les flags fractionn√©"
    print(msg)
    return msg
    
@app.route("/export_fractionne_excel")
def export_fractionne_excel():
    activities = load_activities()
    rows = []
    for a in activities:
        aid = a.get("activity_id")
        label = a.get("is_fractionne_label", "")
        rows.append({"activity_id": aid, "fractionne_label": label})
    df = pd.DataFrame(rows)
    excel_path = "fractionne_labels.xlsx"
    df.to_excel(excel_path, index=False)
    return f"‚úÖ Fichier export√© : {excel_path}"

@app.route("/import_fractionne_excel")
def import_fractionne_excel():
    import pandas as pd
    excel_path = "fractionne_labels.xlsx"
    df = pd.read_excel(excel_path)
    label_map = dict(zip(df["activity_id"], df["fractionne_label"]))

    activities = load_activities()
    changed = 0
    for a in activities:
        aid = a.get("activity_id")
        if aid in label_map and pd.notna(label_map[aid]):
            new_val = bool(label_map[aid])
            if a.get("is_fractionne_label") != new_val:
                a["is_fractionne_label"] = new_val
                changed += 1

    if changed > 0:
        upload_json_content_to_drive(activities, "activities.json")

    return f"‚úÖ {changed} activit√©s mises √† jour depuis Excel"
    
@app.route("/debug_autotrain_status")
def debug_autotrain_status():
    activities = load_activities()

    # m√™mes calculs que les helpers
    meta = _load_last_train_meta()
    last_cnt = meta.get("last_trained_count", 0)
    cur_cnt = len(activities)

    pos = neg = 0
    for a in activities:
        if "is_fractionne_label" in a:
            if bool(a["is_fractionne_label"]): pos += 1
            else: neg += 1

    eligible = (cur_cnt > last_cnt) and (pos >= 8 and neg >= 8)

    return (
        f"AUTO_RETRAIN_XGB={AUTO_RETRAIN_XGB}<br>"
        f"last_trained_count={last_cnt}<br>"
        f"current_activities={cur_cnt}<br>"
        f"new_activities={cur_cnt - last_cnt}<br>"
        f"labels_pos={pos}, labels_neg={neg} (min 8/8)<br>"
        f"eligible={eligible}"
    )
    
@app.route("/force_autotrain_xgb")
def force_autotrain_xgb():
    ok = _retrain_fractionne_model_and_reload()
    return "‚úÖ Auto-train OK" if ok else "‚ùå Auto-train √©chou√©", (200 if ok else 500)




if __name__ == "__main__":
    app.run(debug=True)
